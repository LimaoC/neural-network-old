{"/":{"title":"Neural Network","content":"\nHello, and welcome to my neural network! This is where I store my second brain - filled with notes from my mathematics and computer science degrees.","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/bayes-theorem":{"title":"Bayes' Theorem","content":"\n## Definition\nLet $A$ and $B$ be arbitrary [events](statistics/event.md) such that $\\mathbb{P}(A) \u003e 0$ and $0 \u003c \\mathbb{P}(B) \u003c 1$. Then\n$$\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} = \\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(A)},$$\nusing the definition of [conditional probability](statistics/conditional-probability.md) twice and the [Law of Total Probability](statistics/law-of-total-probability.md).\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/booles-inequality":{"title":"Boole's Inequality","content":"\n## Definition\nFor any [events](statistics/event.md) $A_1, A_2, \\dots$, we have\n$$\\mathbb{P}\\left(\\bigcup_i A_i\\right) \\leq \\sum_i \\mathbb{P}(A_i).$$\nThat is, the probability that at least one of the events occurs is less than or equal to the sum of the probabilities of the individual events.\n\n## Proof\nWe proceed by induction. The $n = 1$ case holds, since $\\mathbb{P}(A_1) \\leq \\mathbb{P}(A_1)$. Then assume the $n = k$ case holds; that is,\n$$\\mathbb{P}\\left(\\bigcup_{i=1}^k A_i\\right) \\leq \\sum_{i=1}^k \\mathbb{P}(A_i).$$\nUsing $\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)$, \n$$\\mathbb{P}\\left(\\bigcup_{i=1}^{k+1} A_i \\right) = \\mathbb{P}\\left(\\bigcup_{i=1}^k A_i \\right) + \\mathbb{P}(A_{k+1}) - \\mathbb{P}\\left(\\left(\\bigcup_{i=1}^n A_i \\right) \\bigcap A_{k+1} \\right),$$\nand since $\\mathbb{P}(E) \\geq 0$ for any event $E$ ([first axiom of probability](statistics/probability-measure.md)),\n$$\\begin{align*}\n\\mathbb{P}\\left(\\bigcup_{i=1}^{k+1} A_i \\right) \u0026\\leq \\mathbb{P}\\left(\\bigcup_{i=1}^k A_i \\right) + \\mathbb{P}(A_{k+1}) \\\\\n\u0026\\leq \\sum_{i=1}^k \\mathbb{P}(A_i) + \\mathbb{P}(A_{k+1}) \\\\\n\u0026= \\sum_{i=1}^{k+1} \\mathbb{P}(A_i).\n\\end{align*}$$\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/conditional-probability":{"title":"Conditional Probability","content":"\n## Definition\nThe [event](statistics/event.md) that $A$ occurs, given that we know $B$ has occurred, is denoted $A | B$. We have\n$$\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)},$$\nwhere $\\mathbb{P}(B \u003e 0)$. \n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/cumulative-distribution-function":{"title":"Cumulative Distribution Function","content":"\n## Definition\nIf $X$ is an arbitrary [random variable](statistics/random-variable.md), then the function\n$$F(x) = \\mathbb{P}(X \\leq x)$$\nis the *(cumulative) distribution function* of $X$. It is often denoted $F_X(x)$.\n\n## Properties\nLet $X$ be a random variable. Then the distribution function $F$ has the following properties:\n1. $F(x) \\to 0$ as $x \\to -\\infty$.\n2. $F(x) \\to 1$ as $x \\to \\infty$.\n3. $F$ is an increasing function; that is $x \u003e y \\implies F(x) \\geq F(y)$.\n4. $F$ is continuous from the right; that is, for all $x$, $F(x + h) \\to F(x)$ as $h \\downarrow 0$ (limit from above).\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/de-morgans-laws":{"title":"De Morgan's Laws","content":"\n## Definition\nLet $\\lbrace A_i \\rbrace$ be a collection of [events](statistics/event.md). Then\n$$\\left(\\bigcup_i A_i\\right)^c = \\bigcap_i A_i^c \\quad\\text{and}\\quad\\left(\\bigcap_i A_i\\right)^c = \\bigcup_i A_i^c.$$\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/disjoint-event":{"title":"Disjoint Event","content":"\n## Definition\nIf $A$ and $B$ are two [events](statistics/event.md) such that they share no outcomes in common, they are said to be *disjoint*, that is $A \\cap B = \\varnothing$. More generally, $A_1, A_2, \\dots$ is said to be *disjoint* if each pair of events $(A_i, A_j$) with $i \\neq j$, is disjoint.\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/event":{"title":"Event","content":"\n## Definition\nAn *event* is any subset of outcomes; that is, any subset of the [sample space](statistics/sample-space.md) $\\Omega$.\n\nIf $A$ is an event, then $A$ complement, denoted $A^c$, is the event that $A$ does not occur.\n\nLet $\\lbrace A_i \\rbrace$ be a collection of events. \\\nThe *union* of $\\lbrace A_i \\rbrace$, denoted $\\cup_i A_i = A_1 \\cup A_2 \\cup \\cdots$, is the event that $A_i$ occurs for at least one value of $i$. \\\nThe *intersection* of $\\lbrace{A_i \\rbrace}$, denoted $\\cap_iA_i = A_1 \\cap A_2 \\cap \\cdots$, is the event that $A_i$ occurs for all values of $i$.\n\nIf $A$ and $B$ are two events such that $A \\neq B$, and all the outcomes in $A$ are also in $B$, then $A$ is a *proper subset* of $B$, denoted $A \\subset B$. If $A$ and $B$ are equal, then $A$ is a *subset* of $B$, denoted $A \\subseteq B$. $A \\subset B$ can also be read as \"$A$ implies $B$\". Note that \"subset\" is often said as a shorthand of \"proper subset\" when $A \\subset B$ is used.\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/expectation":{"title":"Expectation","content":"\n## Definition\nLet $X$ be a [discrete random variable](statistics/random-variable.md) with [probability mass function](statistics/probability-mass-function.md) $f_X(x)$ where $x \\in S$. Then the expected value of $X$, denoted by $\\mathbb{E}(X)$ and commonly shortened to $\\mathbb{E}X$, is given by\n$$\\mathbb{E}(X) = \\sum_{x \\in S} xf_X(x) = \\sum_{x \\in S} x\\mathbb{P}(X = x).$$\n\nIntuitively, the expectation of a random variable $X$ is a weighted average of the values that $X$ takes ($x \\in S$), where the weight is given by the probability of each value.\n\n## Properties\nLet $X$ and $Y$ be two arbitrary random variables. Then\n1. $\\mathbb{E}(aX + b) = a\\mathbb{E}X + b$ for all $a, b$\n2. $\\mathbb{E}(X + Y) = \\mathbb{E}X + \\mathbb{E}Y$\n3. $X \\geq Y \\implies \\mathbb{E}X \\geq \\mathbb{E}Y$\n\n**Proof.** The first property follows from applying [LOTUS](statistics/law-of-the-unconscious-statistician.md). For the second property, first define $X: \\Omega \\to \\mathbb{R}$, $Y: \\Omega \\to \\mathbb{R}$ and $Z(\\omega) = X(\\omega) + Y(\\omega)$. Then\n$$\\begin{align*}\n\\mathbb{E}Z \u0026= \\sum_{\\omega\\in\\Omega} Z(\\omega) \\mathbb{P}(\\lbrace\\omega\\rbrace) \\\\\n\u0026= \\sum_{\\omega\\in\\Omega} (X(\\omega) + Y(\\omega)) \\mathbb{P}(\\lbrace\\omega\\rbrace) \\\\\n\u0026= \\sum_{\\omega\\in\\Omega} X(\\omega) \\mathbb{P}(\\lbrace\\omega\\rbrace) + \\sum_{\\omega\\in\\Omega} Y(\\omega) \\mathbb{P}(\\lbrace\\omega\\rbrace) \\\\\n\u0026= \\mathbb{E}X + \\mathbb{E}Y,\n\\end{align*}$$\nwith the continuous case following analogously with integrals. For the third property, $X \\geq Y \\implies X - Y \\geq 0 \\implies \\mathbb{E}(X - Y) \\geq 0 \\implies \\mathbb{E}X \\geq \\mathbb{E}Y$, where we use the second property in the last step.","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/independence":{"title":"Independence","content":"\n## Definition\nTwo [events](statistics/event.md) $A$ and $B$ are said to be *independent* if $\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)$.\n\nA collection of events $A_1, A_2, \\dots$ is said to be *pairwise independent* if, for all $i \\neq j$, $A_i$ and $A_j$ are independent; that is, $\\mathbb{P}(A_i \\cap A_j) = \\mathbb{P}(A_i)\\mathbb{P}(A_j)$. \\\nThey are said to be *triplewise independent* if, for all distinct $i, j, k$, we have $\\mathbb{P}(A_i \\cap A_j \\cap A_k) = \\mathbb{P}(A_i)\\mathbb{P}(A_j)\\mathbb{P}(A_k)$.\n\nGenerally, they are said to be *mutually independent* if each event is independent of any combination of other events in the collection; that is, $$\\mathbb{P}\\left(\\bigcap_i A_i \\right) = \\prod_i \\mathbb{P}(A_i).$$\n\n## Properties\n\nIf $A$ and $B$ are independent events, then $A^c$ and $B^c$ are independent, i.e. $\\mathbb{P}(A^c \\cap B^c) = \\mathbb{P}(A^c)\\mathbb{P}(B^c)$. \\\n**Proof.** Using [De Morgan's Law](statistics/de-morgans-laws.md), we have $\\mathbb{P}(A^c \\cap B^c) = \\mathbb{P}((A \\cup B)^c)$. Then\n$$\\begin{align*}\n\\mathbb{P}(A^c \\cap B^c) \u0026= 1 - \\mathbb{P}(A \\cup B) \\\\\n\u0026= 1 - [\\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)] \\\\\n\u0026= 1 - [\\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A)\\mathbb{P}(B)] \u0026\u0026 A, B \\text{ independent} \\\\\n\u0026= 1 - \\mathbb{P}(A) - \\mathbb{P}(B) + \\mathbb{P}(A)\\mathbb{P}(B) \\\\\n\u0026= (1 - \\mathbb{P}(A))(1 - \\mathbb{P}(B)) \\\\\n\u0026= \\mathbb{P}(A^c)\\mathbb{P}(B^c).\n\\end{align*}$$\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/law-of-the-unconscious-statistician":{"title":"Law of the Unconscious Statistician","content":"\n\u003e [!abstract] Theorem.\n\u003e \n\u003e (Also known as LOTUS). Let $X$ be a [discrete random variable](statistics/random-variable.md) with [probability mass function](statistics/probability-mass-function.md) $f_X(x)$ where $x \\in S$, and let $g(X)$ be some function of $X$. Then the [expectation](statistics/expectation.md) of $g(X)$ is given by\n\u003e $$\\mathbb{E}(g(X)) = \\sum_{x \\in S} g(x)f_X(x) = \\sum_{x \\in S} g(x) \\mathbb{P}(X = x).$$\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/law-of-total-probability":{"title":"Law of Total Probability","content":"\n## Definition\nIf $A$ and $B$ are [events](statistics/event.md) such that $0 \u003c \\mathbb{P}(B) \u003c 1$, then\n$$\\mathbb{P}(A) = \\mathbb{P}(A|B)\\mathbb{P}(B) + \\mathbb{P}(A|B^c)\\mathbb{P}(B^c).$$\nMore generally, if $\\lbrace B_i \\rbrace$ is a collection of events that form a [partition](statistics/partition.md) of $\\Omega$ such that $\\mathbb{P}(B_i) \u003e 0$ for at least one $i$,  then\n$$\\mathbb{P}(A) = \\sum_i \\mathbb{P}(A|B_i)\\mathbb{P}(B_i) = \\sum_i \\mathbb{P}(A \\cap B_i)$$\nfor any event $A$, by the definition of [conditional probability](statistics/conditional-probability.md).\n\n## Proof\nWe have\n$$A = A \\cap \\Omega = A \\cap (B \\cup B^c) = (A \\cap B) \\cup (A \\cap B^c),$$\nwhere $A \\cap B$ and $A \\cap B^c$ are disjoint events. Then from the [third axiom of probability](statistics/probability-measure.md) and the definition of conditional probability,\n$$\\mathbb{P}(A) = \\mathbb{P}(A \\cap B) + \\mathbb{P}(A \\cap B^c) = \\mathbb{P}(A|B)\\mathbb{P}(B) + \\mathbb{P}(A|B^c)\\mathbb{P}(B^c).$$\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/partition":{"title":"Partition","content":"\n## Definition\n[Events](statistics/event.md) $A_1, A_2, \\dots$ are said to be *exhaustive* if $A_1 \\cup A_2 \\cup \\cdots = \\Omega$, that is at least one $A_i$ must occur.\n\nIf $A_1, A_2, \\dots$ are exhaustive and mutually exclusive (i.e. they cannot occur simultaneously), then they are a *partition* of $\\Omega$ and are said to partition the sample space.\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/probability-mass-function":{"title":"Probability Mass Function","content":"\n## Definition\nIf $X$ is a *discrete [random variable](statistics/random-variable.md)*, then the function\n$$f(x) = \\mathbb{P}(X = x)$$\nis the *probability (mass) function* of $X$. It is often denoted $f_X(x)$.\n\n## Properties\nIf $X$ is a discrete random variable taking values in $S$, then\n1. $f_X(x) \\geq 0$ for all real $x$\n2. $\\sum_{x\\in S} f_X(x) = 1$.\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/probability-measure":{"title":"Probability Measure","content":"\n## Definition\nThe function $\\mathbb{P}$ is called a *probability measure* if it satisfies the following properties, or *axioms*:\n1. $\\mathbb{P}(A) \\geq 0$ for all events $A$\n2. $\\mathbb{P}(\\Omega) = 1$\n3. If $A_1, A_2, \\dots$ are [mutually exclusive events](statistics/event.md), then $\\mathbb{P}(A_1 \\cup A_2 \\cup \\cdots) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\cdots$\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/random-variable":{"title":"Random Variable","content":"\n## Definition\nA random variable $X$ is a function or mapping from the possible outcomes in a [sample space](statistics/sample-space.md) to numerical values, usually the real numbers.\n\nA random variable is said to be *discrete* if it takes values in (i.e. maps to) a countable set.\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null},"/statistics/sample-space":{"title":"Sample Space","content":"\n## Definition\nThe set $\\Omega$ of all possible outcomes of an experiment or random trial is called the *sample space*. \n\nSubsets of the sample space are known as [events](statistics/event.md).\n","lastmodified":"2022-12-27T12:41:03.248333168Z","tags":null}}
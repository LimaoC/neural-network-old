{"/":{"title":"Neural Network","content":"\nHello, and welcome to my neural network! This is where I store my second brain - filled with notes from my mathematics and computer science degrees.","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/bayes-theorem":{"title":"Bayes' Theorem","content":"\n\u003e [!abstract] Theorem.\n\u003e \n\u003e Let $A$ and $B$ be arbitrary [events](statistics/event.md) such that $\\mathbb{P}(A) \u003e 0$ and $0 \u003c \\mathbb{P}(B) \u003c 1$. Then\n\u003e $$\\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)} = \\frac{\\mathbb{P}(A|B)\\mathbb{P}(B)}{\\mathbb{P}(A)},$$\n\u003e using the definition of [conditional probability](statistics/conditional-probability.md) twice and the [Law of Total Probability](statistics/law-of-total-probability.md),\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/booles-inequality":{"title":"Boole's Inequality","content":"\nFor any [events](statistics/event.md) $A_1, A_2, \\dots$, we have\n$$\\mathbb{P}\\left(\\bigcup_i A_i\\right) \\leq \\sum_i \\mathbb{P}(A_i).$$\nThat is, the probability that at least one of the events occurs is less than or equal to the sum of the probabilities of the individual events.\n\n\u003e [!note] Proof.\n\u003e \n\u003e We proceed by induction. The $n = 1$ case holds, since $\\mathbb{P}(A_1) \\leq \\mathbb{P}(A_1)$. \\\n\u003e Assume the $n = k$ case holds; that is,\n\u003e $$\\mathbb{P}\\left(\\bigcup_{i=1}^k A_i\\right) \\leq \\sum_{i=1}^k \\mathbb{P}(A_i).$$\n\u003e Then, using $\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)$, \n\u003e $$\\mathbb{P}\\left(\\bigcup_{i=1}^{k+1} A_i \\right) = \\mathbb{P}\\left(\\bigcup_{i=1}^k A_i \\right) + \\mathbb{P}(A_{k+1}) - \\mathbb{P}\\left(\\left(\\bigcup_{i=1}^n A_i \\right) \\bigcap A_{k+1} \\right),$$\n\u003e and since $\\mathbb{P}(E) \\geq 0$ for any event $E$ (from the [first axiom of probability](statistics/probability-measure.md)),\n\u003e $$\\begin{align*}\n\\mathbb{P}\\left(\\bigcup_{i=1}^{k+1} A_i \\right) \u0026\\leq \\mathbb{P}\\left(\\bigcup_{i=1}^k A_i \\right) + \\mathbb{P}(A_{k+1}) \\\\\n\u0026\\leq \\sum_{i=1}^k \\mathbb{P}(A_i) + \\mathbb{P}(A_{k+1}) \\\\\n\u0026= \\sum_{i=1}^{k+1} \\mathbb{P}(A_i).\n\\end{align*}$$\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/conditional-probability":{"title":"Conditional Probability","content":"\n\u003e [!abstract] Definition.\n\u003e \n\u003e The [event](statistics/event.md) that $A$ occurs, given that we know $B$ has occurred, is denoted $A | B$. We have\n\u003e $$\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)},$$\n\u003e where $\\mathbb{P}(B \u003e 0)$. \n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/de-morgans-laws":{"title":"De Morgan's Laws","content":"\n\u003e [!abstract] Theorem.\n\u003e \n\u003e Let $\\lbrace A_i \\rbrace$ be a collection of [events](statistics/event.md). Then\n\u003e $$\\left(\\bigcup_i A_i\\right)^c = \\bigcap_i A_i^c \\quad\\text{and}\\quad\\left(\\bigcap_i A_i\\right)^c = \\bigcup_i A_i^c.$$\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/disjoint-event":{"title":"Disjoint Event","content":"\n\u003e [!abstract] Definition.\n\u003e \n\u003e If $A$ and $B$ are two [events](statistics/event.md) such that they share no outcomes in common, they are said to be *disjoint*, that is $A \\cap B = \\varnothing$. More generally, $A_1, A_2, \\dots$ is said to be *disjoint* if each pair of events $(A_i, A_j$) with $i \\neq j$, is disjoint.\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/event":{"title":"Event","content":"\n\u003e [!abstract] Definition.\n\u003e \n\u003e An *event* is any subset of outcomes; that is, any subset of the [sample space](statistics/sample-space.md) $\\Omega$.\n\n\u003e [!abstract] Definition.\n\u003e \n\u003e If $A$ is an event, then $A$ complement, denoted $A^c$, is the event that $A$ does not occur.\n\n\u003e [!abstract] Definition.\n\u003e \n\u003e Let $\\lbrace A_i \\rbrace$ be a collection of events. \\\n\u003e The *union* of $\\lbrace A_i \\rbrace$, denoted $\\cup_i A_i = A_1 \\cup A_2 \\cup \\cdots$, is the event that $A_i$ occurs for at least one value of $i$. \\\n\u003e The *intersection* of $\\lbrace{A_i \\rbrace}$, denoted $\\cap_iA_i = A_1 \\cap A_2 \\cap \\cdots$, is the event that $A_i$ occurs for all values of $i$.\n\n\u003e [!abstract] Definition.\n\u003e \n\u003e If $A$ and $B$ are two events such that $A \\neq B$, and all the outcomes in $A$ are also in $B$, then $A$ is a *proper subset* of $B$, denoted $A \\subset B$. If $A$ and $B$ are equal, then $A$ is a *subset* of $B$, denoted $A \\subseteq B$. $A \\subset B$ can also be read as \"$A$ implies $B$\". Note that \"subset\" is often said as as shorthand of \"proper subset\" when $A \\subset B$ is used.\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/independence":{"title":"Independence","content":"\n\u003e [!abstract] Definition.\n\u003e \n\u003e Two [events](statistics/event.md) $A$ and $B$ are said to be *independent* if $\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)$.\n\u003e \n\u003e A collection of events $A_1, A_2, \\dots$ is said to be *pairwise independent* if, for all $i \\neq j$, $A_i$ and $A_j$ are independent; that is, $\\mathbb{P}(A_i \\cap A_j) = \\mathbb{P}(A_i)\\mathbb{P}(A_j)$. \\\n\u003e They are said to be *triplewise independent* if, for all distinct $i, j, k$, we have $\\mathbb{P}(A_i \\cap A_j \\cap A_k) = \\mathbb{P}(A_i)\\mathbb{P}(A_j)\\mathbb{P}(A_k)$.\n\u003e \n\u003e Generally, they are said to be *mutually independent* if each event is independent of any combination of other events in the collection; that is, $$\\mathbb{P}\\left(\\bigcap_i A_i \\right) = \\prod_i \\mathbb{P}(A_i).$$\n\nIf $A$ and $B$ are independent events, then $A^c$ and $B^c$ are independent, i.e. $\\mathbb{P}(A^c \\cap B^c) = \\mathbb{P}(A^c)\\mathbb{P}(B^c)$.\n\u003e [!note] Proof.\n\u003e \n\u003e Using [De Morgan's Law](statistics/de-morgans-laws.md), we have $\\mathbb{P}(A^c \\cap B^c) = \\mathbb{P}((A \\cup B)^c)$. Then\n\u003e $$\\begin{align*}\n\\mathbb{P}(A^c \\cap B^c) \u0026= 1 - \\mathbb{P}(A \\cup B) \\\\\n\u0026= 1 - [\\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)] \\\\\n\u0026= 1 - [\\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A)\\mathbb{P}(B)] \u0026\u0026 A, B \\text{ independent} \\\\\n\u0026= 1 - \\mathbb{P}(A) - \\mathbb{P}(B) + \\mathbb{P}(A)\\mathbb{P}(B) \\\\\n\u0026= (1 - \\mathbb{P}(A))(1 - \\mathbb{P}(B)) \\\\\n\u0026= \\mathbb{P}(A^c)\\mathbb{P}(B^c).\n\\end{align*}$$\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/law-of-total-probability":{"title":"Law of Total Probability","content":"\n\u003e [!abstract] Theorem.\n\u003e \n\u003e If $A$ and $B$ are [events](statistics/event.md) such that $0 \u003c \\mathbb{P}(B) \u003c 1$, then\n\u003e $$\\mathbb{P}(A) = \\mathbb{P}(A|B)\\mathbb{P}(B) + \\mathbb{P}(A|B^c)\\mathbb{P}(B^c).$$\n\u003e More generally, if $\\lbrace B_i \\rbrace$ is a collection of events that form a [partition](statistics/partition.md) of $\\Omega$ such that $\\mathbb{P}(B_i) \u003e 0$ for at least one $i$,  then\n\u003e $$\\mathbb{P}(A) = \\sum_i \\mathbb{P}(A|B_i)\\mathbb{P}(B_i) = \\sum_i \\mathbb{P}(A \\cap B_i)$$\n\u003e for any event $A$, by the definition of [conditional probability](statistics/conditional-probability.md).\n\n\u003e [!note] Proof.\n\u003e \n\u003e We have\n\u003e $$A = A \\cap \\Omega = A \\cap (B \\cup B^c) = (A \\cap B) \\cup (A \\cap B^c),$$\n\u003e where $A \\cap B$ and $A \\cap B^c$ are disjoint events.\n\u003e \n\u003e Then from the [third axiom of probability](statistics/probability-measure.md) and the definition of conditional probability,\n\u003e $$\\mathbb{P}(A) = \\mathbb{P}(A \\cap B) + \\mathbb{P}(A \\cap B^c) = \\mathbb{P}(A|B)\\mathbb{P}(B) + \\mathbb{P}(A|B^c)\\mathbb{P}(B^c).$$\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/partition":{"title":"Partition","content":"\n\u003e [!abstract] Definition.\n\u003e \n\u003e [Events](statistics/event.md) $A_1, A_2, \\dots$ are said to be *exhaustive* if $A_1 \\cup A_2 \\cup \\cdots = \\Omega$, that is at least one $A_i$ must occur. If $A_1, A_2, \\dots$ are exhaustive and mutually exclusive (i.e. they cannot occur simultaneously), then they are a *partition* of $\\Omega$ and are said to partition the sample space.\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/probability-measure":{"title":"Probability Measure","content":"\n\u003e [!abstract] Definition.\n\u003e \n\u003e The function $\\mathbb{P}$ is called a *probability measure* if it satisfies the following properties or *axioms*:\n\u003e 1. $\\mathbb{P}(A) \\geq 0$ for all events $A$\n\u003e 2. $\\mathbb{P}(\\Omega) = 1$\n\u003e 3. If $A_1, A_2, \\dots$ are [mutually exclusive events](statistics/event.md), then $\\mathbb{P}(A_1 \\cup A_2 \\cup \\cdots) = \\mathbb{P}(A_1) + \\mathbb{P}(A_2) + \\cdots$\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null},"/statistics/sample-space":{"title":"Sample Space","content":"\n\u003e [!abstract] Definition.\n\u003e \n\u003e The set $\\Omega$ of all possible outcomes of an experiment or random trial is called the *sample space*. \n\nSubsets of the sample space are known as [events](statistics/event.md).\n","lastmodified":"2022-11-26T11:42:20.920068845Z","tags":null}}
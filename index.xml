<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural Network on</title><link>https://neuralnetwork.limaochang.dev/</link><description>Recent content in Neural Network on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://neuralnetwork.limaochang.dev/index.xml" rel="self" type="application/rss+xml"/><item><title>Markov's Inequality</title><link>https://neuralnetwork.limaochang.dev/statistics/markovs-inequality/</link><pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/markovs-inequality/</guid><description>Definition If $X$ be a nonnegative random variable (i.e. $X \geq 0$) and $a &amp;gt; 0$. Then $$\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a}.</description></item><item><title>Probability Density Function</title><link>https://neuralnetwork.limaochang.dev/statistics/probability-density-function/</link><pubDate>Wed, 28 Dec 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/probability-density-function/</guid><description>Definition If $X: \Omega \to \mathbb{R}$ is a continuous random variable, then the function $f$ such that $f(x) \geq 0$ for all $x \in \Omega$ and $$\mathbb{P}(x \leq X \leq y) = \int_x^y f(u)du$$ for all $x \leq y$, that is, the probability that $X$ takes on values in the interval $[x, y]$ is given by the area under the graph of the pdf from $x$ to $y$.</description></item><item><title>Moment</title><link>https://neuralnetwork.limaochang.dev/statistics/moment/</link><pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/moment/</guid><description>Definition Let $X$ be an arbitrary random variable. Then the n-th moment of $X$ is given by $\mathbb{E}(X^n)$.
For example, the expectation of $X$ is the first moment of $X$, and the variance of $X$ is the second moment of $X$ minus the square of the first moment of $X$.</description></item><item><title>Variance</title><link>https://neuralnetwork.limaochang.dev/statistics/variance/</link><pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/variance/</guid><description>Definition Let $X$ be an arbitrary random variable. Then the variance of $X$, denoted $\text{Var}(X)$, is given by $$\text{Var}(X) = \mathbb{E}(X - \mu_X)^2,$$ and quantities the spread of the distribution of $X$, or the variation about the mean, $\mu_X$.</description></item><item><title>Expectation</title><link>https://neuralnetwork.limaochang.dev/statistics/expectation/</link><pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/expectation/</guid><description>Definition Let $X$ be a discrete random variable with probability mass function $f_X(x)$ where $x \in S$. Then the expected value of $X$, denoted by $\mathbb{E}(X)$ and commonly shortened to $\mathbb{E}X$, is given by $$\mathbb{E}(X) = \sum_{x \in S} xf_X(x) = \sum_{x \in S} x\mathbb{P}(X = x).</description></item><item><title>Law of the Unconscious Statistician</title><link>https://neuralnetwork.limaochang.dev/statistics/law-of-the-unconscious-statistician/</link><pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/law-of-the-unconscious-statistician/</guid><description>Definition Let $X$ be a discrete random variable with support $S$ and with probability mass function $f_X(x)$ where $x \in S$, and let $g(X)$ be some function of $X$.</description></item><item><title>Cumulative Distribution Function</title><link>https://neuralnetwork.limaochang.dev/statistics/cumulative-distribution-function/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/cumulative-distribution-function/</guid><description>Definition If $X$ is an arbitrary random variable, then the function $$F(x) = \mathbb{P}(X \leq x)$$ is the (cumulative) distribution function of $X$.</description></item><item><title>Probability Mass Function</title><link>https://neuralnetwork.limaochang.dev/statistics/probability-mass-function/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/probability-mass-function/</guid><description>Definition If $X$ is a discrete random variable, then the function $$f(x) = \mathbb{P}(X = x)$$ is the probability (mass) function of $X$.</description></item><item><title>Random Variable</title><link>https://neuralnetwork.limaochang.dev/statistics/random-variable/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/random-variable/</guid><description>Definition A random variable $X$ is a function or mapping from the possible outcomes in a sample space to numerical values, usually the real numbers.</description></item><item><title>Bayes' Theorem</title><link>https://neuralnetwork.limaochang.dev/statistics/bayes-theorem/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/bayes-theorem/</guid><description>Definition Let $A$ and $B$ be arbitrary events such that $\mathbb{P}(A) &amp;gt; 0$ and $0 &amp;lt; \mathbb{P}(B) &amp;lt; 1$. Then $$\mathbb{P}(B|A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(A)} = \frac{\mathbb{P}(A|B)\mathbb{P}(B)}{\mathbb{P}(A)},$$ using the definition of conditional probability twice and the Law of Total Probability.</description></item><item><title>Disjoint Event</title><link>https://neuralnetwork.limaochang.dev/statistics/disjoint-event/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/disjoint-event/</guid><description>Definition If $A$ and $B$ are two events such that they share no outcomes in common, they are said to be disjoint, that is $A \cap B = \varnothing$.</description></item><item><title>Partition</title><link>https://neuralnetwork.limaochang.dev/statistics/partition/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/partition/</guid><description>Definition Events $A_1, A_2, \dots$ are said to be exhaustive if $A_1 \cup A_2 \cup \cdots = \Omega$, that is at least one $A_i$ must occur.</description></item><item><title>Conditional Probability</title><link>https://neuralnetwork.limaochang.dev/statistics/conditional-probability/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/conditional-probability/</guid><description>Definition The event that $A$ occurs, given that we know $B$ has occurred, is denoted $A | B$. We have $$\mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},$$ where $\mathbb{P}(B &amp;gt; 0)$.</description></item><item><title>Independence</title><link>https://neuralnetwork.limaochang.dev/statistics/independence/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/independence/</guid><description>Definition Two events $A$ and $B$ are said to be independent if $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$.
A collection of events $A_1, A_2, \dots$ is said to be pairwise independent if, for all $i \neq j$, $A_i$ and $A_j$ are independent; that is, $\mathbb{P}(A_i \cap A_j) = \mathbb{P}(A_i)\mathbb{P}(A_j)$.</description></item><item><title>Law of Total Probability</title><link>https://neuralnetwork.limaochang.dev/statistics/law-of-total-probability/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/law-of-total-probability/</guid><description>Definition If $A$ and $B$ are events such that $0 &amp;lt; \mathbb{P}(B) &amp;lt; 1$, then $$\mathbb{P}(A) = \mathbb{P}(A|B)\mathbb{P}(B) + \mathbb{P}(A|B^c)\mathbb{P}(B^c).$$ More generally, if $\lbrace B_i \rbrace$ is a collection of events that form a partition of $\Omega$ such that $\mathbb{P}(B_i) &amp;gt; 0$ for at least one $i$, then $$\mathbb{P}(A) = \sum_i \mathbb{P}(A|B_i)\mathbb{P}(B_i) = \sum_i \mathbb{P}(A \cap B_i)$$ for any event $A$, by the definition of conditional probability.</description></item><item><title>Boole's Inequality</title><link>https://neuralnetwork.limaochang.dev/statistics/booles-inequality/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/booles-inequality/</guid><description>Definition For any events $A_1, A_2, \dots$, we have $$\mathbb{P}\left(\bigcup_i A_i\right) \leq \sum_i \mathbb{P}(A_i).$$ That is, the probability that at least one of the events occurs is less than or equal to the sum of the probabilities of the individual events.</description></item><item><title>De Morgan's Laws</title><link>https://neuralnetwork.limaochang.dev/statistics/de-morgans-laws/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/de-morgans-laws/</guid><description>Definition Let $\lbrace A_i \rbrace$ be a collection of events. Then $$\left(\bigcup_i A_i\right)^c = \bigcap_i A_i^c \quad\text{and}\quad\left(\bigcap_i A_i\right)^c = \bigcup_i A_i^c.</description></item><item><title>Event</title><link>https://neuralnetwork.limaochang.dev/statistics/event/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/event/</guid><description>Definition An event is any subset of outcomes; that is, any subset of the sample space $\Omega$.
If $A$ is an event, then $A$ complement, denoted $A^c$, is the event that $A$ does not occur.</description></item><item><title>Probability Measure</title><link>https://neuralnetwork.limaochang.dev/statistics/probability-measure/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/probability-measure/</guid><description>Definition The function $\mathbb{P}$ is called a probability measure if it satisfies the following properties, or axioms:
$\mathbb{P}(A) \geq 0$ for all events $A$ $\mathbb{P}(\Omega) = 1$ If $A_1, A_2, \dots$ are mutually exclusive events, then $\mathbb{P}(A_1 \cup A_2 \cup \cdots) = \mathbb{P}(A_1) + \mathbb{P}(A_2) + \cdots$</description></item><item><title>Sample Space</title><link>https://neuralnetwork.limaochang.dev/statistics/sample-space/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/sample-space/</guid><description>Definition The set $\Omega$ of all possible outcomes of an experiment or random trial is called the sample space.</description></item></channel></rss>
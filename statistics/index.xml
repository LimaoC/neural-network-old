<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Statistics on</title><link>https://neuralnetwork.limaochang.dev/statistics/</link><description>Recent content in Statistics on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 27 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://neuralnetwork.limaochang.dev/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Cumulative Distribution Function</title><link>https://neuralnetwork.limaochang.dev/statistics/cumulative-distribution-function/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/cumulative-distribution-function/</guid><description>[!abstract] Definition. If $X$ is an arbitrary random variable, then the function $$F(x) = \mathbb{P}(X \leq x)$$ is the (cumulative) distribution function of $X$.</description></item><item><title>Probability Mass Function</title><link>https://neuralnetwork.limaochang.dev/statistics/probability-mass-function/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/probability-mass-function/</guid><description>[!abstract] Definition.
If $X$ is a discrete random variable, then the function $$f(x) = \mathbb{P}(X = x)$$ is the probability (mass) function of $X$.</description></item><item><title>Random Variable</title><link>https://neuralnetwork.limaochang.dev/statistics/random-variable/</link><pubDate>Sat, 26 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/random-variable/</guid><description>[!abstract] Definition.
A random variable $X$ is a function or mapping from the possible outcomes in a sample space to numerical values, usually the real numbers.</description></item><item><title>Bayes' Theorem</title><link>https://neuralnetwork.limaochang.dev/statistics/bayes-theorem/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/bayes-theorem/</guid><description>[!abstract] Theorem.
Let $A$ and $B$ be arbitrary events such that $\mathbb{P}(A) &amp;gt; 0$ and $0 &amp;lt; \mathbb{P}(B) &amp;lt; 1$.</description></item><item><title>Disjoint Event</title><link>https://neuralnetwork.limaochang.dev/statistics/disjoint-event/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/disjoint-event/</guid><description>[!abstract] Definition.
If $A$ and $B$ are two events such that they share no outcomes in common, they are said to be disjoint, that is $A \cap B = \varnothing$.</description></item><item><title>Partition</title><link>https://neuralnetwork.limaochang.dev/statistics/partition/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/partition/</guid><description>[!abstract] Definition.
Events $A_1, A_2, \dots$ are said to be exhaustive if $A_1 \cup A_2 \cup \cdots = \Omega$, that is at least one $A_i$ must occur.</description></item><item><title>Conditional Probability</title><link>https://neuralnetwork.limaochang.dev/statistics/conditional-probability/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/conditional-probability/</guid><description>[!abstract] Definition.
The event that $A$ occurs, given that we know $B$ has occurred, is denoted $A | B$. We have $$\mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)},$$ where $\mathbb{P}(B &amp;gt; 0)$.</description></item><item><title>Independence</title><link>https://neuralnetwork.limaochang.dev/statistics/independence/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/independence/</guid><description>[!abstract] Definition.
Two events $A$ and $B$ are said to be independent if $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$.
A collection of events $A_1, A_2, \dots$ is said to be pairwise independent if, for all $i \neq j$, $A_i$ and $A_j$ are independent; that is, $\mathbb{P}(A_i \cap A_j) = \mathbb{P}(A_i)\mathbb{P}(A_j)$.</description></item><item><title>Law of Total Probability</title><link>https://neuralnetwork.limaochang.dev/statistics/law-of-total-probability/</link><pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/law-of-total-probability/</guid><description>[!abstract] Theorem.
If $A$ and $B$ are events such that $0 &amp;lt; \mathbb{P}(B) &amp;lt; 1$, then $$\mathbb{P}(A) = \mathbb{P}(A|B)\mathbb{P}(B) + \mathbb{P}(A|B^c)\mathbb{P}(B^c).</description></item><item><title>Boole's Inequality</title><link>https://neuralnetwork.limaochang.dev/statistics/booles-inequality/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/booles-inequality/</guid><description>For any events $A_1, A_2, \dots$, we have $$\mathbb{P}\left(\bigcup_i A_i\right) \leq \sum_i \mathbb{P}(A_i).$$ That is, the probability that at least one of the events occurs is less than or equal to the sum of the probabilities of the individual events.</description></item><item><title>De Morgan's Laws</title><link>https://neuralnetwork.limaochang.dev/statistics/de-morgans-laws/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/de-morgans-laws/</guid><description>[!abstract] Theorem.
Let $\lbrace A_i \rbrace$ be a collection of events. Then $$\left(\bigcup_i A_i\right)^c = \bigcap_i A_i^c \quad\text{and}\quad\left(\bigcap_i A_i\right)^c = \bigcup_i A_i^c.</description></item><item><title>Event</title><link>https://neuralnetwork.limaochang.dev/statistics/event/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/event/</guid><description>[!abstract] Definition.
An event is any subset of outcomes; that is, any subset of the sample space $\Omega$.
[!</description></item><item><title>Probability Measure</title><link>https://neuralnetwork.limaochang.dev/statistics/probability-measure/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/probability-measure/</guid><description> [!abstract] Definition.
The function $\mathbb{P}$ is called a probability measure if it satisfies the following properties or axioms:
$\mathbb{P}(A) \geq 0$ for all events $A$ $\mathbb{P}(\Omega) = 1$ If $A_1, A_2, \dots$ are mutually exclusive events, then $\mathbb{P}(A_1 \cup A_2 \cup \cdots) = \mathbb{P}(A_1) + \mathbb{P}(A_2) + \cdots$</description></item><item><title>Sample Space</title><link>https://neuralnetwork.limaochang.dev/statistics/sample-space/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuralnetwork.limaochang.dev/statistics/sample-space/</guid><description>[!abstract] Definition.
The set $\Omega$ of all possible outcomes of an experiment or random trial is called the sample space.</description></item></channel></rss>